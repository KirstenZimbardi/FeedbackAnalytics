---
title: "UQM paper outputs"
author: "KZ"
date: "29 June 2015"
output: html_document
---
  

```{r, echo=FALSE, message=FALSE}
#clean workspace
rm(list=ls())

#packages
require(reshape2)
require(dplyr)

#load functions
source("UQM functions.R")
source("HelperFunctions.R")

ProjectID = read.csv("./Data Stage 1/ProjectIDs.csv")
projects()
graphs()


#load data
folder = "./Data Stage 2/"
demog = uqm.csv("demog")
AcP = uqm.csv("AcPdf")
FbPsum = uqm.csv("FbPsummary")
FbP = uqm.csv("FbPdf")
FbU = uqm.csv("FbUdf")
AcPFbUbins = uqm.csv("AcPFbUbins")
SubID.index = uqm.csv("SubIDindex")
mt = uqm.csv("MarkingTimes")

#Generalising
demog = gen(demog, 13, 14)
AcP = gen(AcP, 7, 8)
FbP = gen(FbP, 15, 16)
FbU = gen(FbU, 22, 23)
AcPFbUbins = gen(AcPFbUbins, 7, 8)
SubID.index = gen(SubID.index, 4, 5)
mt = gen(mt, 7, 8)
#FbPsum does not inlude project details, need to merge in from AcP
#FbP.sum = merge(FbPsum, AcP, by="SubmissionID", all.y=T)
#FbP.sum[which(is.na(FbP.sum[,2])),] #bunch of Submissions for which there are marks but no meta-data on Fb provision - possible error in generating FbPsummary?
#ignoring this for now and just generating Fig 2-4 based on data that is in FbPsummary.csv
FbP.AcP = merge(FbPsum, AcP, by="SubmissionID")

```


#Methods
##Student cohort  
```{r, echo=FALSE}
df = subset(demog, Course.Grade !="withrawn")
temp = df %>% group_by(course, sem) %>% tally()
temp2 = as.data.frame(round(prop.table(table(df[,5])),2)*100)

df[,4] = as.Date(df[,4], "%d/%M/%Y")
sem1 = as.Date("2013-03-01")
#t5 = t5 %>% mutate(finished.p = round((finished/Recording)*100),0)
df2 = df %>% select(Date.of.Birth) 
df2 = df2 %>% mutate(age = (sem1 - Date.of.Birth))
df2$age = as.numeric(df2[,2])%/%365
#df2[1:10,]
#table(df2[,3])
bins = c(0, 16, 21, 60)
df2$age.bin = cut(df2[,2], bins)
#head(df2, n=20)
temp3 = as.data.frame(round(prop.table(table(df2$age.bin)),2)*100)

temp4 = as.data.frame(round(prop.table(table(df$International.Indicator.Code)),2)*100)
#temp4 = as.data.frame(round(prop.table(table(df$International.Indicator.Code)),2)*100)
#round(prop.table(table(df$UQ.Translated.Base.OP)),2)*100

bin.op = c(0, 8, 20)

temp5 = as.data.frame(round(prop.table(table(cut(df$UQ.Translated.Base.OP, bin.op))),2)*100)

```


During the period of this study, there were `r temp[1,3]` and `r temp[2,3]` students enrolled in the level 1 course, and `r temp[3,3]` and `r temp[3,3]` students enrolled in the level 2 course in 2013 Semesters 1 and 2, respectively. There were slightly more `r tolower(temp2[1,1])` (`r temp2[1,2]`%) than `r tolower(temp2[2,1])` (`r temp2[2,2]`%)  students, the vast majority (`r temp3[2,2]`%) were 17 – 21 years old, domestic students (`r temp4[1,2]`%), with high- to mid-range university entry scores (`r temp5[1,2]`%). 


**_need to get program data to check the following:_**    
Just over half (53%) of the students taking these courses were enrolled in a three-year Bachelor of Science (BSc) or four-year dual degree combining BSc with another degree, or the Bachelor of Biomedical Science (four year research-focused program). An additional 21% were enrolled in one of four sports science degrees, and 12% were enrolled in the combined BSc/Medical program (in which students undertake an accelerated two-year BSc program and then enter a four-year, graduate entry Medical program). The remaining students were enrolled in a large range of single and dual degrees (e.g. Arts, Engineering), specialty degrees and applied science degrees.  


##Data collection and analysis  
```{r, echo=FALSE}
temp = length(unique(AcP$SubmissionID))
```
A total of `r temp` reports submitted and marked through FACS in semester 1 and 2, 2013 were used for this study. The modality, length (in time or number of words) and position of each feedback annotation provided on all reports was recorded, and the time taken to mark each collated. Clickstream data logs were collected as students accessed their marked documents, providing information on opening dates and durations, and student interaction with the feedback annotations. Data on student academic performance for each report were also collated. Throughout this study, quantitative analyses were performed using R 3.1.1 (R Development Core Team, Auckland, NZ). The results were expressed as mean and standard error of the mean (SEM), and were considered significant if p<0.05. 

#Results and Discussion    
```{r, echo=FALSE, warning=FALSE, message=FALSE}
temp = dcast(AcP, course + sem ~ report)
#temp
df = subset(demog, Course.Grade !="withrawn")
temp2 = as.data.frame(df %>% group_by(course) %>% tally())
```

The increasing availability of online technologies, which allow the provision of multimodal feedback annotations that can be generalised or situated, has certainly increased the variety and flexibility of feedback delivery options (refs). However, it has also increased the variability of feedback provision and changed the way in which students interact with feedback, leading to potentially greater variability in student outcomes (refs). The primary aims of this study were to understand how the different modalities of feedback available through online technologies affected feedback provisionboth across successive student assessment tasks and within large cohorts with multiple markers, and to examine the way students interacted with the feedback provided. Analyses of FACS data from laboratory reports submitted for assessment (Table 1) in two biomedical science courses in level 1 (n = `r temp2[1,2]` students) and level 2 (n = `r temp2[2,2]`), in Semesters 1 and 2, 2013, have shown that there are significant differences in the ways in which markers use the different modalities of feedback (Figure 2-4). In addition, the data demonstrates that there are substantial differences in the way students interact with their marked reports and feedback within them across the semesters (Figures 5-8).

Table 1: Number of assignments processed through FACS  
```{r, echo=FALSE}
temp
```
NB Report 4 for the level 1 course in semester 2 was administered by another academic department which did not participate in the FACS trial.


```{r, echo=FALSE}
stats = stats.mean.sem(FbP, 18, 4)
stats = round(stats,1)

stats2 = stats.mean.sem(FbP.AcP, 4, 15)
stats2 = round(stats2,1)
#stats2
stats3 = stats.mean.sem(FbP.AcP, 5, 15)
stats3 = round(stats3,1)
#stats3

df = subset(FbP, AnnotType == "Recording" | AnnotType == "Text")
t = with(df, t.test(WordCount ~ AnnotType))
t$p.value = tp(t)
#t$p.value

df2 = subset(FbP.AcP, course == "Level 1")
df2 = df2 %>% select(SubmissionID, Recording, Text)
#head(df2)
df2 = melt(df2, id=c("SubmissionID"), variable.name = "AnnotType", value.name = "annot.num")
t2 = with(df2, t.test(annot.num ~ AnnotType))
t2$p.value = tp(t2)

df2 = subset(FbP.AcP, course == "Level 2")
df2 = df2 %>% select(SubmissionID, Recording, Text)
#head(df2)
df2 = melt(df2, id=c("SubmissionID"), variable.name = "AnnotType", value.name = "annot.num")
t3 = with(df2, t.test(annot.num ~ AnnotType))
t3$p.value = tp(t3)

df = subset(FbP, AnnotType == "Recording" | AnnotType == "Text")
df.1 = subset(df, course == "Level 1")
t4 = with(df.1, t.test(WordCount ~ AnnotType))
t4$p.value = tp(t4)

df.2 = subset(df, course == "Level 2")
t5 = with(df.2, t.test(WordCount ~ AnnotType))
t5$p.value = tp(t5)

```

##Feedback Provision
A randomly selected sample of 160 audio annotations were transcribed and their word count determined. Talking speed was relatively stable across audio annotations, as they contained on average 164+/-6 words per minute, which is also consistent with previous reports of 450-500 words per 3 minute audio feedback comment (Brearley and Cullen 2012). To allow comparison between word length of typed and audio annotations, the length (in minutes) of each audio annotation was multiplied by 164, with audio annotations on average containing significantly more words (`r stats[3,1]`+/-`r stats[3,2]` words) than typed annotations (`r stats[4,1]`+/-`r stats[4,2]` words; p`r t$p.value`) (Figure 4). In the first years’ reports, markers provided on average a significantly greater number of typed annotations (`r stats3[1,1]`+/-`r stats3[1,2]`) than audio annotations (`r stats2[1,1]`+/-`r stats2[1,2]`; p`r t2$p.value`; Figure 2). This was reversed on the second years’ reports, where markers provided significantly more audio annotations (`r stats2[2,1]`+/-`r stats2[2,2]`) than typed annotations (`r stats3[2,1]`+/-`r stats3[2,2]`; p`r t3$p.value`; Figure 2). However, for both first (p`r t4$p.value`) and second (p`r t5$p.value`) years’ reports, the total amount of feedback provided in audio annotations, in terms of word length, was significantly greater than typed annotations (Figure 3). 


```{r, echo=FALSE}
df = FbP.AcP
t = with(df, t.test(Recording ~ course))
t$p.value = tp(t)

df2 = subset(FbP, AnnotType == "Recording")
t2 = with(df2, t.test(WordCount ~ course))
t2$p.value = tp(t2)

stats = stats.mean.sem(df2, 18, 15)
stats = round(stats,1)

df3 = subset(FbP, AnnotType == "Text")
t3 = with(df3, t.test(WordCount ~ course))
t3$p.value = tp(t3)

stats3 = stats.mean.sem(df3, 18, 15)
stats3 = round(stats3,1)

t4 = with(FbP, t.test(WordCount ~ course))
t4$p.value = tp(t4)

stats4 = stats.mean.sem(FbP, 18, 15)
stats4 = round(stats4,1)

t5 = with(mt, t.test(marking.min ~ course))
t5$p.value = tp(t5)

stats5 = stats.mean.sem(mt, 10, 7)
stats5 = round(stats5,1)


```


When comparing between year levels, it was found that markers provided significantly more audio annotations on second years’ work on average than on the first years’ reports (p`r t$p.value`; Figure 2), the annotations were also slightly, but significantly longer (`r stats[2,1]`+/-`r stats[2,2]` words; p`r t2$p.value`) than those provided to first year students (`r stats[1,1]`+/-`r stats[1,2]` words). In contrast, on the first year students reports markers provided more, significantly longer (`r stats3[1,1]`+/-`r stats3[1,2]` words) text annotations than for second years reports, who had fewer, shorter text annotations (`r stats3[2,1]`+/-`r stats3[2,2]` words; p`r t3$p.value`). Despite this, the total amount of feedback provided on second years’ work (`r stats4[2,1]`+/-`r stats4[2,2]` words) was significantly greater than that on first years’ reports (`r stats4[1,1]`+/-`r stats4[1,2]` words; p`r t4$p.value`; Figure 4A & B), *with audio annotations making up a significantly larger proportion of the total feedback provided on the second years’ reports* [DELETE? REPETITVE?]. Reports from second year students also took longer to mark (`r stats5[2,1]`+/-`r stats5[2,2]` minutes) than first years’ reports (`r stats5[1,1]`+/-`r stats5[1,2]` minutes; p`r t5$p.value`). This may be related to the differences in task design between first and second year, with the second years’ reports being of greater average length (1909+/-150 words) than the first years’ reports (1269+/-99 words), or may reflect that the higher expectations of scientific reasoning for second year students require more detailed explanation by markers (ref APE paper). 


```{r, echo=FALSE, fig.width=10, fig.height=5}

plot.topper.dual()
plot.mean.sem(FbP.AcP, 4, 14, 20, report.col, report.names, "Number of audio annotations per report")
plot.mean.sem(FbP.AcP, 5, 14, 20, report.col, report.names, "Number of text annotations per report")
legend.top(sem.names,  kz.col)

temp = as.data.frame(table(FbP.AcP[,15], FbP.AcP[,17]))

```

Figure 2: Number of audio and text annotations for level 1 course (Report 0: n=`r temp[1,3]`, Report 1: n=`r temp[3,3]`, Report 2: n=`r temp[5,3]`, Report 3: n=`r temp[7,3]`) and for the level 2 course (Report 1: n=`r temp[4,3]`, Report 2: n=`r temp[6,3]`). Shown is the A)  mean+/-SEM number of audio annotations per report across two semesters of level 1 and 2 subjects; B)  mean+/-SEM number of text annotations per report across two semesters of level 1 and 2 subjects.


```{r, echo=FALSE}

df = subset(FbP.AcP, report != "Report 0")
df = df %>% select(SubmissionID, report, Recording, Text)
#head(df)
dfm = melt(df, id=c("SubmissionID", "report"))
#head(dfm)
dfm12 = subset(dfm, report == "Report 1" | report == "Report 2")
t1 = t.test(dfm12[,4] ~ dfm12[,2])
t1$p.value = tp(t1)
stats1 = stats.mean.sem(dfm12, 4, 2)
stats1 = round(stats1,1)

dfm23 = subset(dfm, report == "Report 3" | report == "Report 2")
t2 = t.test(dfm23[,4] ~ dfm23[,2])
t2$p.value = tp(t2)
stats2 = stats.mean.sem(dfm23, 4, 2)
stats2 = round(stats2,1)

df2 = FbP %>% filter(AnnotType == "Recording") %>% select(SubmissionID, course, sem, report, project, Duration.sec, WordCount)
stats3 = stats.mean.sem(df2, 6, 2:3)
stats3 = round(stats3,1)
stats3minmean = min(stats3[,1:2])
w = which(stats3[,1:2] == stats3minmean)
s = as.matrix(stats3[,3:4])
stats3maxmean = max(stats3[,1:2])
w2 = which(stats3[,1:2] == stats3maxmean)
#s[w2]
#(stats3minmean/60)*164
#(stats3maxmean/60)*164
am = round(mean(df2[,6]),1) #ave audio duration (sec) all annotations
as = round(se(df2, 6),1)
#mean(df2[,7]) #ave audio length (words) all annotations
#mean(FbP.AcP[,6]) #ave total feedback (words) per report
#mean(FbP.AcP[,8]) #ave total audio length (words) per report
#mean(FbP.AcP[,9]) #ave total text length (words) per report

```

With the exception of the formative task, markers tended to provide more audio annotations on the earliest reports in each year and semester, with the amount of audio feedback provided then declining on subsequent reports across each semester (Figure 2A; p`r t1$p.value`). Interestingly, despite this decline, the average length of each in situ audio annotation was relatively consistent, with averages across semesters and year levels *varying between `r stats3minmean`+/-`r s[w]` and `r stats3maxmean`+/-`r s[w2]` seconds per annotation* [?OR JUST GIVE OVERALL AVERAGE: `r am`+/-`r as` seconds per annotation]. With an average talking speed of 164 words/minute, this would suggest that markers tend to give feedback in approximately 80 word ‘sound bites’, effectively a short paragraph of information. This contrasts with other studies where a single audio annotation is given, as these tend to be considerably longer (Ribchester, France, and Wakefield 2008; Gould and Day 2013), but may equate to a similar amount of audio feedback being provided overall on equivalent tasks (Ice et al. 2010; Lunt and Curran 2010).


```{r, echo=FALSE}

df2 = FbP %>% filter(AnnotType == "Text") %>% select(SubmissionID, course, sem, report, project, WordCount)
stats2 = stats.mean.sem(df2, 6, c(2,4))
stats2 = round(stats2,1)

df2a = df2 %>% filter(course == "Level 1" & sem == "Semester 1") 
#head(df2a)
stats2a = stats.mean.sem(df2a, 6, 4)
stats2a = round(stats2a,1)
t2a = with(df2a, aov(WordCount ~ report))
#summary(t2a)
p2a = ap(t2a)
tuk2a = TukeyHSD(t2a)
#tuk2a
#rownames(tuk2a[[1]])
#tuk2a[[1]][19:24]

df2b = df2 %>% filter(course == "Level 1" & sem == "Semester 2") 
#head(df2b)
stats2b = stats.mean.sem(df2b, 6, 4)
stats2b = round(stats2b,1)
t2b = with(df2b, aov(WordCount ~ report))
#summary(t2b)
p2b = ap(t2b)
tuk2b = TukeyHSD(t2b)
#tuk2b
#rownames(tuk2b[[1]])
#tuk2b[[1]][10:12]

df2c = df2 %>% filter(course == "Level 1" & report != "Report 0") 
#head(df2c)
stats2c = stats.mean.sem(df2c, 6, 4)
stats2c = round(stats2c,1)
t2c = with(df2c, aov(WordCount ~ sem * report))
#summary(t2c)
p2c.sem = summary(t2c)[[1]][1,5]
p2c.report = summary(t2c)[[1]][2,5]
p2c.iteraction = summary(t2c)[[1]][3,5]

stats2c2 = stats.mean.sem(df2c, 6, c(3,4))
stats2c2 = round(stats2c2,1)

stats2c3 = stats.mean.sem(df2c, 6, 3)
stats2c3 = round(stats2c3,1)
t2c3 = t.test(df2c[,6] ~ df2c[,3])
#t2c3$p.value


df3c = df2 %>% filter(report == "Report 1" | report == "Report 2") 
#head(df3c)
t3c = with(df3c, aov(WordCount ~ course * report))
#summary(t3c)
p3c.course = summary(t3c)[[1]][1,5]
p3c.report = summary(t3c)[[1]][2,5]
p3c.iteraction = summary(t3c)[[1]][3,5]
if (round(p3c.course,3) <= 0.001) { p3c.course = "<0.001" }
if (round(p3c.report,3) <= 0.001) { p3c.report = "<0.001" }

df4 = subset(FbP.AcP, course == "Level 2")
df4 = df4 %>% select(SubmissionID, course, sem, report, project, Text, txt.words)
#head(df4)
t4 = with(df4, t.test(Text ~ report))
t4$p.value = tp(t4)

stats4 = stats.mean.sem(df4, 6, 4)
stats4 = round(stats4,1)

```


However, while audio annotations were relatively consistent in length, the typed annotations were not. The first year students in semester 1 received longer typed annotations on their early reports, with the longest typed comments appearing on the formative task (`r stats2a[1,1]`+/-`r stats2a[1,2]` words) and declining thereafter (Report 1: `r stats2a[2,1]`+/-`r stats2a[2,2]` words; Report 2: `r stats2a[3,1]`+/-`r stats2a[3,2]` words; Report 3: `r stats2a[4,1]`+/-`r stats2a[4,2]` words; p`r p2a`; Figure 3). In semester 2, where no formative task existed, *the typed annotations were longer than for the equivalent task in the earlier semester but* [DELETE - NOT SIG DIFF BETWEEN SEMESTERS (P=`r round(p2c.sem,3)`), AND NO SIG INTERACTION (P=`r round(p2c.iteraction,4)`)] declined in a similar way (Report 1: `r stats2b[1,1]`+/-`r stats2b[1,2]` words; Report 2: `r stats2b[2,1]`+/-`r stats2b[2,2]` words; Report 3: `r stats2b[3,1]`+/-`r stats2b[3,2]` words; p`r p2b`; Figure 3). *These differences between semesters suggest that there was an impact of the formative task.Potentially it gave markers an additional opportunity for feedback, allowing them to help the students to establish their writing skills early, then to focus on specific areas for improvement in latter tasks, whereas in the second semester these purposes had to be addressed across fewer tasks. This is supported by the finding that,* [DELETE - NOT SIG DIFF] Collectively, the amount of typed feedback provided across the first years’ reports was similar in each semester (Semester 1: `r stats2c3[1,1]`+/-`r stats2c3[1,2]` words; Semester 2: `r stats2c3[2,1]`+/-`r stats2c3[2,2]` words; p=`r round(t2c3$p.value, 3)`). *For the second years’ reports, the typed annotations were significantly fewer in number and shorter  (Report 1: `r stats2[2,2]`+/-`r stats2[2,6]` words; Report 2: `r stats2[2,3]`+/-`r stats2[2,7]` words, p`r p3c.course`) than those provided on the first years’ reports.* [DELETE - REPEATS SECTION IN 2ND PARAGRAPH OF FEEDBACK PROVISION SECTION] For the second years’ reports, the number (Report 1: `r stats3[1,1]`+/-`r stats3[1,2]` words; Report 2: `r stats3[2,1]`+/-`r stats3[2,2]` words, p`r t4$p.value`) and length (Report 1: `r stats2[2,2]`+/-`r stats2[2,6]` words; Report 2: `r stats2[2,3]`+/-`r stats2[2,7]` words, p`r p3c.report`) of typed annotations declined across the two reports. *This may reflect the preponderance of audio comments on the second years’ reports, with the brief typed comments likely to be primarily minor error corrections (Merry and Orsmond 2008).* [DELETE - NO LONGER SUPPORTED BY STAT'S] 


```{r, echo=FALSE}
df = FbP[,c(1,4:9,14:18)]
#head(df)   

df2 = FbU %>% select(SubmissionID, Pages)
df2 = unique(df2)
df2 = remover(df2, 2)
df3 = merge(df, df2)

#with(df3, table(Pages, Page))
#with(df3, round(addmargins(prop.table(table(Pages)))*100),4)
#with(df3, round(addmargins(prop.table(table(Page)))*100),4)

df3 = df3 %>% mutate(annot.pos = (Page/Pages)*100)
df3$annot.pos = round(df3$annot.pos,0)

#default.plot()
#hist(df3$annot.pos)

firstpage = round((prop.table(table(df3$Page == 1))*100),0)

df3 = df3 %>% mutate(lastpage = (Page == Pages)*100)
lastpage = round((prop.table(table(df3$lastpage))*100),2)
#lastpage[2]

nearlastpage = round((prop.table(with(df3, table(annot.pos >= 80)))*100),0)

```

The availability of in situ feedback possible with this marking tool was well utilised by markers, with annotations, whether typed or audio, being placed primarily in situ, and very few reports receiving ‘summary’ annotations either on the first (`r firstpage[2]`%) or last (`r lastpage[2]`%) page of the report text. This meant that each feedback annotation was placed near the specific portion of student work to which it referred. Previous studies using audio annotations have highlighted that the separation of comments from student work, for example when an overall audio comment is made or when audio and text files are separate, is viewed as a disadvantage by students (Ribchester, France, and Wakefield 2008; Rodway-Dyer, Dunne, and Newcombe 2009). In this regard, the advent of completely online assessment submission and marking systems, which allow the embedding of feedback regardless of modality, represent a notable improvement in feedback provision.


```{r, echo=FALSE, fig.width=10, fig.height=5}

audio = subset(FbP, AnnotType == "Recording")
text = subset(FbP, AnnotType == "Text")

plot.topper.dual()
par(mar = c(4.5, 5.1, 2.1, 1.5))
plot.mean.sem(audio, 18, 14, 170, report.col, report.names, "Number of words per audio annotation \n per report")
plot.mean.sem(text, 18, 14, 17, report.col, report.names, "Number of words per text annotation \n per report")

legend.top(sem.names,  kz.col)

temp = as.data.frame(table(FbP.AcP[,15], FbP.AcP[,17]))

#Kay wanted range of audio durations (sec) for later text
#temp3 = as.data.frame(tapply(FbP[,12], FbP[,14], mean, na.rm=T))
#temp4 = NULL
#for(i in 1:11)
#  temp4[[i]] = se(subset(FbP, project == project.names.formative[i]), 12)
#temp3
#temp4  
  
```

Figure 3: Number of words provided in each audio and text annotation for level 1 course (Report 0: n=`r temp[1,3]`, Report 1: n=`r temp[3,3]`, Report 2: n=`r temp[5,3]`, Report 3: n=`r temp[7,3]`) and for the level 2 course (Report 1: n=`r temp[4,3]`, Report 2: n=`r temp[6,3]`). Shown is the A) mean+/-SEM number of words in each audio annotation in reports across two semesters and levels; B) mean+/-SEM number of words in each text annotation in reports across two semesters and two levels. *NB The y-axis scale for A is 10x the y-axis scale for B.*


```{r, echo=FALSE, fig.width=10, fig.height=5}

plot.topper.dual()

plot.mean.sem(FbP.AcP, 8, 14, 1700, report.col, report.names, "Number of words in audio annotations per report")
plot.mean.sem(FbP.AcP, 9, 14, 170, report.col, report.names, "Number of words in text annotations per report")

legend.top(sem.names,  kz.col)

temp = as.data.frame(table(FbP.AcP[,15], FbP.AcP[,17]))

```

Figure 4: Total amount of feedback provided in audio and text feedback for level 1 course (Report 0: n=`r temp[1,3]`, Report 1: n=`r temp[3,3]`, Report 2: n=`r temp[5,3]`, Report 3: n=`r temp[7,3]`) and for the level 2 course (Report 1: n=`r temp[4,3]`, Report 2: n=`r temp[6,3]`). Shown is the A) mean+/-SEM number of words in audio annotations in reports across two semesters and levels; B) mean+/-SEM number of words in text annotations in reports across two semesters and two levels. *NB The y-axis scale for A is 10x the y-axis scale for B.*


```{r, echo=FALSE, message=F, warning=F}

look = AcPFbUbins %>% select(StudentID, course, project, OpenDuration.min)
look = remover(look, 4)
u = which(look$OpenDuration.min == 0)
look = look[-u,]

d = dcast(look, StudentID ~ course, sum)

biol = d[which(d[,2] > 0),1]
biom = d[which(d[,3] > 0),1]

biol.all = demog %>% filter(course == "Level 1") %>% select(StudentID, sem, Course.Grade)
biom.all = demog %>% filter(course == "Level 2") %>% select(StudentID, sem, Course.Grade)

biol.looked = round(prop.table(table(biol.all[,1] %in% biol))*100,0)
biom.looked = round(prop.table(table(biom.all[,1] %in% biom))*100,0)


temp = with(subset(AcPFbUbins, course == "Level 1" & report == "Report 3"), round((prop.table(table(open.bin)))*100),2)
temp2 = 100-temp[4]

temp3 = with(subset(AcPFbUbins, course == "Level 2"), round((prop.table(table(open.bin, report),2))*100),2)
temp4 = 100-temp3[4,1]
temp5 = 100-temp3[4,2]

```

##Feedback use 
**_Proportion of students viewing report feedback_**  
The vast majority of the first year students opened their marked reports (`r biol.looked[2]`%), although the proportion who did so tended to decline slightly across the semester, with the final report being opened by fewer students (`r temp2`%; Figure 5). Surprisingly, the reverse was true in second year students. In comparison to the first year students, fewer second year students opened any of their reports (`r biom.looked[2]`%), with only `r temp4`% opening their first report, but `r temp5`% opening their second report, a similar proportion to the first year students. Notably this pattern was consistent in each semester (Figure 5). It is difficult to identify the cause of this difference between first and second year students, it may reflect a lower engagement with assessment by second year students (Loughlin et al. 2013) or could be that the additional opportunities that the they had during class to gain verbal feedback on their first report meant that they were less reliant on the feedback provided on the report documents. 


```{r, echo=FALSE, fig.width=10, fig.height=5}
looked = as.data.frame(addmargins(prop.table(table(AcPFbUbins[,6], AcPFbUbins[,11]),1)))
looked = round(100-(looked[37:47,3]*100),2)
#looked

default.plot()
barplot(looked, names = report.names, las=2, col=report.col, ylim=c(0,100), ylab="Number of Students (% of cohort)", axis.lty=1.0)
legend.top(sem.names,  kz.col)

temp = as.data.frame(table(AcP[,6]))
```


Figure 5: The proportion of students who looked at their feedback (shown as the proportion of students who received feedback in each cohort). Number of students in each cohort: Level 1 Semester 1: n=`r temp[1,2]`; Level 1 Semester 2: n=`r temp[5,2]`; Level 2 Semester 1: n=`r temp[8,2]`; Level 2 Semester 2: n=`r temp[10,2]`. 


```{r, echo=FALSE}
df = AcPFbUbins %>% filter(open.bin != "unopened")
#with(df, tapply(OpenDuration.min, final, mean, na.rm=T))
stats = stats.mean.sem(df, 10, 12)
stats = round(stats,0)

stats2 = stats.mean.sem(df, 10, c(12,7,8))
stats2 = round(stats2,0)

finalminmean = min(stats2[1,1:4])
w = which(stats2[1,1:4] == finalminmean)
s = as.matrix(stats2[1,5:8])
finalmaxmean = max(stats2[1,1:4])
w2 = which(stats2[1,1:4] == finalmaxmean)

nonfinalminmean = min(stats2[2,1:4])
w3 = which(stats2[2,1:4] == nonfinalminmean)
s2 = as.matrix(stats2[2,5:8])
nonfinalmaxmean = max(stats2[2,1:4])
w4 = which(stats2[2,1:4] == nonfinalmaxmean)

stats3maxmean = max(stats3[,1:2])
w2 = which(stats3[,1:2] == stats3maxmean)
#s[w2]

df2 = df %>% filter(final == "nonfinal")
nf = as.data.frame(with(df2, round(prop.table(table(open.bin, final))*100,0)))


```

**_Feedback viewing duration and pause times_**   
In terms of the duration for which student had reports open, in each semester and year level, the duration declined markedly to *30-90 minutes* [?REPLACE WITH:] (`r stats[1,1]`+/-`r stats[1,2]` minutes) [?OR REPLACE WITH:] `r finalminmean`+/-`r s[w]` to `r finalmaxmean`+/-`r s[w2]` minutes for the final report, compared with much longer open durations for the earlier 'non-final' reports (`r round(stats[2,1]/60,1)`+/-`r stats[2,2]/60` hours) [??OR AS RANGE] (`r round(nonfinalminmean/60,1)`+/-`r round(s2[w3]/60,1)` to `r round(nonfinalmaxmean/60,1)`+/-`r round(s2[w4]/60,1)`) [?TO REPLACE:] *which ranged between 3-7 hours on average across the two courses* (Figure 6). When the patterns of open duration across the cohorts were examined in more detail (Figure 7), it was apparent that students interacted with their marked reports in different ways. For the non-final reports in each semester, of the students who opened their marked reports, the majority were divided approximately equally between those who opened their report for between one minute and an hour (`r nf[2,1]`%), and those who opened them for greater than an hour (`r nf[1,1]`%), with just a small number (`r nf[3,1]`%) who opened their reports for 1 minute or less. It is possible that these durations represent different categories of students, students who work through the feedback thoroughly; students who work through the feedback thoroughly and also use it directly to inform their subsequent report writing; and a small tail of students who glance through their reports very quickly, perhaps primarily to check their grade. 

**_need to add in P groups and convert dates_**  
This categorisation is supported by the timing of openings where it can be seen that there is a cluster of openings occurring shortly after the release of the marked reports (Figure X) and another cluster in the period 48 hours prior to the due date for the next report (Figure X) with the latter group being of longer open durations. It is also supported by the pattern of open durations for the final reports from each semester, which have on average, much shorter open durations (Figure 6) and far greater proportions of students falling into the categories of shorter open durations, of less than one minute (X%), or between one minute and one hour (X%; Figure 7B & D). The duration and timing of students interaction with feedback occurring on the non-final tasks suggests that the students perceive these tasks to be sufficiently similar to one another for the feedback to be useful for the subsequent tasks (Boud and Molloy 2013) but also suggests that one of the key drivers for student interaction with feedback is the immediacy of its use on similar assessment tasks. 


```{r, echo=FALSE, fig.width=10, fig.height=5}

default.plot()
plot.mean.sem.adj(AcPFbUbins, 10, 6, 8, report.col, report.names, "Open duration (hours)", 60)
legend.top(sem.names,  kz.col)

temp = as.data.frame(table(AcPFbUbins[,7], AcPFbUbins[,9]))

```


Figure 6: The duration students had their report feedback open for the level 1 course (Report 0: n=`r temp[1,3]`, Report 1: n=`r temp[3,3]`, Report 2: n=`r temp[5,3]`, Report 3: n=`r temp[7,3]`) and for the level 2 course (Report 1: n=`r temp[4,3]`, Report 2: n=`r temp[6,3]`). Data are expressed as the mean+/-SEM time for which each report was open, in hours.


```{r, echo=FALSE, fig.width=12, fig.height=5}

x = my.logs(-4, 9)
p = project.names.formative[c(1,4,8,9)]

default.plot()
par(mfrow=c(1,2))

for(i in 1:2) { 
  hist(log(subset(AcPFbUbins, project == p[i])[,10]), main = "", ylab = "Number of students", xlab = "Open duration (log scale)", xlim=c(-4,9),  ylim=c(0, 200), axes=F)
axis(1, at=seq(-4, 9, 1), labels = x)
axis(2)
}
for(i in 3:4) {
  hist(log(subset(AcPFbUbins, project == p[i])[,10]), main = "", ylab = "Number of students", xlab = "Open duration (log scale)", xlim=c(-4,9),  ylim=c(0, 50), axes=F)
axis(1, at=seq(-4, 9, 1), labels = x)
axis(2)
}


temp = as.data.frame(table(AcPFbUbins[,6]))

```

Figure 7: The number of students who had their feedback open for various durations following the Level 1 Semester 1 course A) formative report (Report 0: n=`r temp[1,2]`), and B) final report (Report 3: n=`r temp[4,2]`), and the Level 2 Semester 1 course C) first report (Report 1: n=`r temp[8,2]`), and C) final report (Report 2: n=`r temp[9,2]`). Data are presented as a frequency histogram showing the number of student across the log transformed open duration. 



```{r, echo=FALSE}

df = FbU %>% filter (msec.B4.scroll > 0)

stats = stats.mean.sem(df, 26, 27)
stats = as.data.frame(round(stats,2))

#df2 = df %>% filter(course == "Level 1")
#stats2 = stats.mean.sem(df2, 25, c(23:24))
#stats2 = round(stats2,0)
#df3 = df %>% filter(course == "Level 2")
#stats3 = stats.mean.sem(df3, 25, c(23:24))
#stats3 = round(stats3,0)
#round(stats2/60,2)
#round(stats3/60,2)

pause.bin = c(0, 1, 180, max(df$pause.sec))
pauses = as.data.frame(round(prop.table(table(cut(df$pause.sec, pause.bin)))*100,0))

```

However the analytics used in this study are able to go beyond simple duration of openings, with the “clickstream” data providing the first insights into the temporal patterns of student interactions with feedback, through an examination of the duration for which students paused until the next ‘click’. These clicks represent student interactions with the report document, such as selecting a position within it, opening an audio annotation or scrolling. The average duration of these pauses was `r stats[2,1]`+/-`r stats[2,2]` minutes for non-final reports, and `r stats[1,1]`+/-`r stats[1,2]` minutes for final reports. The vast majority of these pauses (`r pauses[2,2]`%) fell between 1 second and 3 minutes, with very few pauses exceeding one hour (`r pauses[3,2]`%; Figure 9). This finding suggests that, despite the often considerable duration for which students had their reports open, they were spending much of this time on active interaction with the report, rather than simply leaving it open for extended periods. In addition, this pattern of behaviour was very consistent across all non-final reports at both year levels, such that it is likely that this represents the “normal” pattern by which students interact with in situ feedback. 

```{r, echo=FALSE, fig.width=10, fig.height=5}

default.plot()
plot.mean.sem(FbU, 26, 21, 4, report.col, report.names, "Pause duration (min)")
legend.top(sem.names,  kz.col)

temp = as.data.frame(table(AcPFbUbins[,7], AcPFbUbins[,9]))

```


Figure 8: Duration for which students pause between clicks when interacting with feedback for the level 1 course (Report 0: n=`r temp[1,3]`, Report 1: n=`r temp[3,3]`, Report 2: n=`r temp[5,3]`, Report 3: n=`r temp[7,3]`) and for the level 2 course (Report 1: n=`r temp[4,3]`, Report 2: n=`r temp[6,3]`). Data are expressed as the mean+/-SEM time that students paused between clicks for each report, in minutes


```{r, echo=FALSE, fig.width=10, fig.height=5}
df = FbU %>% filter (msec.B4.scroll > 0)

low = -5
high = 6
x = my.logs(low, high)

default.plot()
h = hist(log(df$pause.min), main = "", ylab = "Number of pauses", xlab = "Pause duration (log scale)", xlim=c(low, high),  ylim=c(0, 150000), axes=F)
axis(1, at=seq(low, high, 1), labels = x)
axis(2)

temp = length(unique(df$SubmissionID))
```

Figure 9: Frequency histogram of the duration students pause between clicks when interacting with feedback across the corpus of reports (n=`r temp` reports). Data are expressed as the number of times students paused between clicks for each duration (x scale is log transformed ranging from msec to hours).


```{r, echo=FALSE, fig.width=10, fig.height=5}
df = FbU %>% filter (msec.B4.scroll > 0)
df1 = df %>% filter(final == "nonfinal" )
df2 = df %>% filter(final == "final" )

low = -5
high = 6
x = my.logs(low, high)

default.plot()
par(mfrow=c(1,2))

h = hist(log(df1$pause.min), main = "", ylab = "Number of pauses", xlab = "Pause duration (log scale)", xlim=c(low, high),  ylim=c(0, 120000), axes=F)
axis(1, at=seq(low, high, 1), labels = x)
axis(2)

h = hist(log(df2$pause.min), main = "", ylab = "Number of pauses", xlab = "Pause duration (log scale)", xlim=c(low, high),  ylim=c(0, 120000), axes=F)
axis(1, at=seq(low, high, 1), labels = x)
axis(2)

temp1 = length(unique(df1$SubmissionID))
temp2 = length(unique(df2$SubmissionID))


```


Figure 9: Frequency histogram of the duration students pause between clicks when interacting with feedback for the A) non-final reports (n=`r temp1` reports) and B) final reports (n=`r temp2` reports). Data are expressed as the number of times students paused between clicks for each duration (x scale is log transformed ranging from msec to hours).


```{r, echo=FALSE, fig.width=10, fig.height=5}
df = FbU %>% filter (msec.B4.scroll > 0)
df1 = df %>% filter(final == "nonfinal" )
df2 = df %>% filter(final == "final" )

low = -5
high = 6
x = my.logs(low, high)
y = seq(0, 40, 10)
default.plot()
par(mfrow=c(1,2))

hist(log(df1$pause.min), main = "", ylab = "Proportion of pauses (%)", xlab = "Pause duration (log scale)", xlim=c(low, high),  ylim=c(0, 0.4), axes=F, prob = T)
axis(1, at=seq(low, high, 1), labels = x)
axis(2, at=seq(0, 0.4, 0.1), labels = y)

hist(log(df2$pause.min), main = "", ylab = "Proportion of pauses (%)", xlab = "Pause duration (log scale)", xlim=c(low, high),  ylim=c(0, 0.4), axes=F, prob = T)
axis(1, at=seq(low, high, 1), labels = x)
axis(2, at=seq(0, 0.4, 0.1), labels = y)

temp1 = length(unique(df1$SubmissionID))
temp2 = length(unique(df2$SubmissionID))


```


Figure 9: Frequency histogram of the duration students pause between clicks when interacting with feedback for the A) non-final reports (n=`r temp1` reports) and B) final reports (n=`r temp2` reports). Data are expressed as the proportion of pauses students took between clicks, ranging from milliseconds to hours (x scale is log transformed to improve clarity).


```{r, echo=FALSE, fig.width=10, fig.height=5}

default.plot()
plot.mean.sem(AcP, 5, 6, 100, report.col, report.names, "Report mark (%)")
legend.top(sem.names,  kz.col)

temp = as.data.frame(table(AcP[,7], AcP[,9]))

```


Figure 10: Overall final mark for reports in the level 1 course (Report 0: n=`r temp[1,3]`, Report 1: n=`r temp[3,3]`, Report 2: n=`r temp[5,3]`, Report 3: n=`r temp[7,3]`) and for the level 2 course (Report 1: n=`r temp[4,3]`, Report 2: n=`r temp[6,3]`). Data are expressed as the mean+/-SEM mark that students achieved for each report, as a percentage.


```{r, echo=FALSE}
#proportion of audio played
#see
#"UQM paper - audio use.Rmd"
```


