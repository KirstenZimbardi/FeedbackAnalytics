---
title: "UQM paper outputs"
author: "KZ"
date: "29 June 2015"
output: html_document
---

Last modified on `r date()`  

```{r, echo=FALSE, message=FALSE}
#clean workspace
rm(list=ls())

#packages
require(reshape2)
require(dplyr)

#load functions
source("UQM functions.R")
source("HelperFunctions.R")

ProjectID = read.csv("./Data Stage 1/ProjectIDs.csv")
projects()
graphs()


#load data
folder = "./Data Stage 2/"
demog = uqm.csv("demog")
AcP = uqm.csv("AcPdf")
FbPsum = uqm.csv("FbPsummary")
FbP = uqm.csv("FbPdf")
FbU = uqm.csv("FbUdf")
AcPFbUbins = uqm.csv("AcPFbUbins")
SubID.index = uqm.csv("SubIDindex")
mt = uqm.csv("MarkingTimes")

#Generalising
demog = gen(demog, 13, 14)
AcP = gen(AcP, 7, 8)
FbP = gen(FbP, 15, 16)
FbU = gen(FbU, 22, 23)
AcPFbUbins = gen(AcPFbUbins, 7, 8)
SubID.index = gen(SubID.index, 4, 5)
mt = gen(mt, 7, 8)
#FbPsum does not inlude project details, need to merge in from AcP
#FbP.sum = merge(FbPsum, AcP, by="SubmissionID", all.y=T)
#FbP.sum[which(is.na(FbP.sum[,2])),] #bunch of Submissions for which there are marks but no meta-data on Fb provision - possible error in generating FbPsummary?
#ignoring this for now and just generating Fig 2-4 based on data that is in FbPsummary.csv
FbP.AcP = merge(FbPsum, AcP, by="SubmissionID")

```


#Methods
##Student cohort  
```{r, echo=FALSE}
df = subset(demog, Course.Grade !="withrawn")
temp = df %>% group_by(course, sem) %>% tally()
temp2 = as.data.frame(round(prop.table(table(df[,5])),2)*100)

df[,4] = as.Date(df[,4], "%d/%M/%Y")
sem1 = as.Date("2013-03-01")
#t5 = t5 %>% mutate(finished.p = round((finished/Recording)*100),0)
df2 = df %>% select(Date.of.Birth) 
df2 = df2 %>% mutate(age = (sem1 - Date.of.Birth))
df2$age = as.numeric(df2[,2])%/%365
#df2[1:10,]
#table(df2[,3])
bins = c(0, 16, 21, 60)
df2$age.bin = cut(df2[,2], bins)
#head(df2, n=20)
temp3 = as.data.frame(round(prop.table(table(df2$age.bin)),2)*100)

temp4 = as.data.frame(round(prop.table(table(df$International.Indicator.Code)),2)*100)
#temp4 = as.data.frame(round(prop.table(table(df$International.Indicator.Code)),2)*100)
#round(prop.table(table(df$UQ.Translated.Base.OP)),2)*100

bin.op = c(0, 8, 20)

temp5 = as.data.frame(round(prop.table(table(cut(df$UQ.Translated.Base.OP, bin.op))),2)*100)

```


During the period of this study, there were `r temp[1,3]` and `r temp[2,3]` students enrolled in the level 1 course, and `r temp[3,3]` and `r temp[4,3]` students enrolled in the level 2 course in 2013 Semesters 1 and 2, respectively. There were slightly more `r tolower(temp2[1,1])` (`r temp2[1,2]`%) than `r tolower(temp2[2,1])` (`r temp2[2,2]`%)  students, the vast majority (`r temp3[2,2]`%) were 17 – 21 years old, domestic students (`r temp4[1,2]`%), with high- to mid-range university entry scores (`r temp5[1,2]`%). 


**_need to get program data to check the following:_**    
Just over half (53%) of the students taking these courses were enrolled in a three-year Bachelor of Science (BSc) or four-year dual degree combining BSc with another degree, or the Bachelor of Biomedical Science (four year research-focused program). An additional 21% were enrolled in one of four sports science degrees, and 12% were enrolled in the combined BSc/Medical program (in which students undertake an accelerated two-year BSc program and then enter a four-year, graduate entry Medical program). The remaining students were enrolled in a large range of single and dual degrees (e.g. Arts, Engineering), specialty degrees and applied science degrees.  


##Data collection and analysis  
```{r, echo=FALSE}
temp = length(unique(AcP$SubmissionID))
```
A total of `r temp` reports submitted and marked through FACS in semester 1 and 2, 2013 were used for this study. The modality, length (in time or number of words) and position of each feedback annotation provided on all reports was recorded, and the time taken to mark each collated. Clickstream data logs were collected as students accessed their marked documents, providing information on opening dates and durations, and student interaction with the feedback annotations. Data on student academic performance for each report were also collated. Throughout this study, quantitative analyses were performed using R 3.1.1 (R Development Core Team, Auckland, NZ). The results were expressed as mean and standard error of the mean (SEM), and were considered significant if p<0.05. 

#Results and Discussion    
```{r, echo=FALSE, warning=FALSE, message=FALSE}
temp = dcast(AcP, course + sem ~ report)
#temp
df = subset(demog, Course.Grade !="withrawn")
temp2 = as.data.frame(df %>% group_by(course) %>% tally())
```

The increasing availability of online technologies, which allow the provision of multimodal feedback annotations that can be generalised or situated, has certainly increased the variety and flexibility of feedback delivery options (refs). However, it has also increased the variability of feedback provision and changed the way in which students interact with feedback, leading to potentially greater variability in student outcomes (refs). The primary aims of this study were to understand how the different modalities of feedback available through online technologies affected feedback provisionboth across successive student assessment tasks and within large cohorts with multiple markers, and to examine the way students interacted with the feedback provided. Analyses of FACS data from laboratory reports submitted for assessment (Table 1) in two biomedical science courses in level 1 (n = `r temp2[1,2]` students) and level 2 (n = `r temp2[2,2]`), in Semesters 1 and 2, 2013, have shown that there are significant differences in the ways in which markers use the different modalities of feedback (Figure 2-4). In addition, the data demonstrates that there are substantial differences in the way students interact with their marked reports and feedback within them across the semesters (Figures 5-8).

Table 1: Number of assignments processed through FACS  
```{r, echo=FALSE}
temp
```
NB Report 4 for the level 1 course in semester 2 was administered by another academic department which did not participate in the FACS trial.


```{r, echo=FALSE}
stats = stats.mean.sem(FbP, 18, 4)
stats = round(stats,1)

stats2 = stats.mean.sem(FbP.AcP, 4, 15)
stats2 = round(stats2,1)
#stats2
stats3 = stats.mean.sem(FbP.AcP, 5, 15)
stats3 = round(stats3,1)
#stats3

df = subset(FbP, AnnotType == "Recording" | AnnotType == "Text")
t = with(df, t.test(WordCount ~ AnnotType))
t$p.value = tp(t)
#t$p.value

df2 = subset(FbP.AcP, course == "Level 1")
df2 = df2 %>% select(SubmissionID, Recording, Text)
#head(df2)
df2 = melt(df2, id=c("SubmissionID"), variable.name = "AnnotType", value.name = "annot.num")
t2 = with(df2, t.test(annot.num ~ AnnotType))
t2$p.value = tp(t2)

df2 = subset(FbP.AcP, course == "Level 2")
df2 = df2 %>% select(SubmissionID, Recording, Text)
#head(df2)
df2 = melt(df2, id=c("SubmissionID"), variable.name = "AnnotType", value.name = "annot.num")
t3 = with(df2, t.test(annot.num ~ AnnotType))
t3$p.value = tp(t3)

df = subset(FbP, AnnotType == "Recording" | AnnotType == "Text")
df.1 = subset(df, course == "Level 1")
t4 = with(df.1, t.test(WordCount ~ AnnotType))
t4$p.value = tp(t4)

df.2 = subset(df, course == "Level 2")
t5 = with(df.2, t.test(WordCount ~ AnnotType))
t5$p.value = tp(t5)

```

##Feedback Provision
A randomly selected sample of 160 audio annotations were transcribed and their word count determined. Talking speed was relatively stable across audio annotations, as they contained on average 164+/-6 words per minute, which is also consistent with previous reports of 450-500 words per 3 minute audio feedback comment (Brearley and Cullen 2012). To allow comparison between word length of typed and audio annotations, the length (in minutes) of each audio annotation was multiplied by 164, with audio annotations on average containing significantly more words (`r stats[3,1]`+/-`r stats[3,2]` words) than typed annotations (`r stats[4,1]`+/-`r stats[4,2]` words; p`r t$p.value`) (Figure 4). In the first years’ reports, markers provided on average a significantly greater number of typed annotations (`r stats3[1,1]`+/-`r stats3[1,2]`) than audio annotations (`r stats2[1,1]`+/-`r stats2[1,2]`; p`r t2$p.value`; Figure 2). This was reversed on the second years’ reports, where markers provided significantly more audio annotations (`r stats2[2,1]`+/-`r stats2[2,2]`) than typed annotations (`r stats3[2,1]`+/-`r stats3[2,2]`; p`r t3$p.value`; Figure 2). However, for both first (p`r t4$p.value`) and second (p`r t5$p.value`) years’ reports, the total amount of feedback provided in audio annotations, in terms of word length, was significantly greater than typed annotations (Figure 3). 


```{r, echo=FALSE}
df = FbP.AcP
t = with(df, t.test(Recording ~ course))
t$p.value = tp(t)

df2 = subset(FbP, AnnotType == "Recording")
t2 = with(df2, t.test(WordCount ~ course))
t2$p.value = tp(t2)

stats = stats.mean.sem(df2, 18, 15)
stats = round(stats,1)

df3 = subset(FbP, AnnotType == "Text")
t3 = with(df3, t.test(WordCount ~ course))
t3$p.value = tp(t3)

stats3 = stats.mean.sem(df3, 18, 15)
stats3 = round(stats3,1)

t4 = with(FbP.AcP, t.test(total.words ~ course))
t4$p.value = tp(t4)

stats4 = stats.mean.sem(FbP.AcP, 6, 15)
stats4 = round(stats4,0)

t5 = with(mt, t.test(marking.min ~ course))
t5$p.value = tp(t5)

stats5 = stats.mean.sem(mt, 10, 7)
stats5 = round(stats5,1)


```


When comparing between year levels, it was found that markers provided significantly more audio annotations on second years’ work on average than on the first years’ reports (p`r t$p.value`; Figure 2), the annotations were also significantly longer (`r stats[2,1]`+/-`r stats[2,2]` words; p`r t2$p.value`) than those provided to first year students (`r stats[1,1]`+/-`r stats[1,2]` words). In contrast, on the first year students reports markers provided more, significantly longer (`r stats3[1,1]`+/-`r stats3[1,2]` words) text annotations than for second years reports, who had fewer, shorter text annotations (`r stats3[2,1]`+/-`r stats3[2,2]` words; p`r t3$p.value`). Despite this, the total amount of feedback provided on second years’ work (`r stats4[2,1]`+/-`r stats4[2,2]` words) was significantly greater than that on first years’ reports (`r stats4[1,1]`+/-`r stats4[1,2]` words; p`r t4$p.value`; Figure 4A & B), *with audio annotations making up a significantly larger proportion of the total feedback provided on the second years’ reports* [DELETE? REPETITVE?]. Reports from second year students also took longer to mark (`r stats5[2,1]`+/-`r stats5[2,2]` minutes) than first years’ reports (`r stats5[1,1]`+/-`r stats5[1,2]` minutes; p`r t5$p.value`). This may be related to the differences in task design between first and second year, with the second years’ reports being of greater average length (1909+/-150 words) than the first years’ reports (1269+/-99 words), and/or may reflect that the higher expectations of scientific reasoning for second year students require more detailed explanation by markers (Zimbardi et al. 2013). 


```{r, echo=FALSE, fig.width=10, fig.height=5}

plot.topper.dual()
plot.mean.sem(FbP.AcP, 4, 14, 20, report.col, report.names, "Number of audio annotations per report")
plot.mean.sem(FbP.AcP, 5, 14, 20, report.col, report.names, "Number of typed annotations per report")
legend.top(sem.names,  kz.col)

temp = as.data.frame(table(FbP.AcP[,15], FbP.AcP[,17]))

#stats.mean.sem(FbP.AcP, 5, 15)

```

Figure 2: Number of audio and text annotations for level 1 course (Report 0: n=`r temp[1,3]`, Report 1: n=`r temp[3,3]`, Report 2: n=`r temp[5,3]`, Report 3: n=`r temp[7,3]`) and for the level 2 course (Report 1: n=`r temp[4,3]`, Report 2: n=`r temp[6,3]`). Shown is the A)  mean+/-SEM number of audio annotations per report across two semesters of level 1 and 2 subjects; B)  mean+/-SEM number of text annotations per report across two semesters of level 1 and 2 subjects.


```{r, echo=FALSE}

df = subset(FbP.AcP, report != "Report 0")
df = df %>% select(SubmissionID, report, Recording, Text)
#head(df)
dfm = melt(df, id=c("SubmissionID", "report"))
#head(dfm)
dfm12 = subset(dfm, report == "Report 1" | report == "Report 2")
t1 = t.test(dfm12[,4] ~ dfm12[,2])
t1$p.value = tp(t1)
stats1 = stats.mean.sem(dfm12, 4, 2)
stats1 = round(stats1,1)

dfm23 = subset(dfm, report == "Report 3" | report == "Report 2")
t2 = t.test(dfm23[,4] ~ dfm23[,2])
t2$p.value = tp(t2)
stats2 = stats.mean.sem(dfm23, 4, 2)
stats2 = round(stats2,1)

df2 = FbP %>% filter(AnnotType == "Recording") %>% select(SubmissionID, course, sem, report, project, Duration.sec, WordCount)
stats3 = stats.mean.sem(df2, 6, 2:3)
stats3 = round(stats3,1)
stats3minmean = min(stats3[,1:2])
w = which(stats3[,1:2] == stats3minmean)
s = as.matrix(stats3[,3:4])
stats3maxmean = max(stats3[,1:2])
w2 = which(stats3[,1:2] == stats3maxmean)
#s[w2]
#(stats3minmean/60)*164
#(stats3maxmean/60)*164
am = round(mean(df2[,6]),1) #ave audio duration (sec) all annotations
as = round(se(df2, 6),1)
#mean(df2[,7]) #ave audio length (words) all annotations
#mean(FbP.AcP[,6]) #ave total feedback (words) per report
#mean(FbP.AcP[,8]) #ave total audio length (words) per report
#mean(FbP.AcP[,9]) #ave total text length (words) per report

```

With the exception of the formative task (Report 0), markers tended to provide more audio annotations on the earliest reports in each year and semester, with the amount of audio feedback provided then declining on subsequent reports across each semester (Figure 2A; p`r t1$p.value`). Interestingly, despite this decline, the average length of each **in situ** audio annotation was relatively consistent, with averages across semesters and year levels at `r am`+/-`r as` seconds per annotation. With an average talking speed of 164 words/minute, this would suggest that markers tend to give feedback in approximately 80 word ‘sound bites’, effectively a short paragraph of information. This contrasts with other studies where a single audio annotation is given, as these tend to be considerably longer (Ribchester, France, and Wakefield 2008; Gould and Day 2013), but may equate to a similar amount of audio feedback being provided overall on equivalent tasks (Ice et al. 2010; Lunt and Curran 2010).


```{r, echo=FALSE}

df2 = FbP %>% filter(AnnotType == "Text") %>% select(SubmissionID, course, sem, report, project, WordCount)
stats2 = stats.mean.sem(df2, 6, c(2,4))
stats2 = round(stats2,1)

df2a = df2 %>% filter(course == "Level 1" & sem == "Semester 1") 
#head(df2a)
stats2a = stats.mean.sem(df2a, 6, 4)
stats2a = round(stats2a,1)
t2a = with(df2a, aov(WordCount ~ report))
#summary(t2a)
p2a = ap(t2a)
tuk2a = TukeyHSD(t2a)
#tuk2a
#rownames(tuk2a[[1]])
#tuk2a[[1]][19:24]

df2b = df2 %>% filter(course == "Level 1" & sem == "Semester 2") 
#head(df2b)
stats2b = stats.mean.sem(df2b, 6, 4)
stats2b = round(stats2b,1)
t2b = with(df2b, aov(WordCount ~ report))
#summary(t2b)
p2b = ap(t2b)
tuk2b = TukeyHSD(t2b)
#tuk2b
#rownames(tuk2b[[1]])
#tuk2b[[1]][10:12]

df2c = df2 %>% filter(course == "Level 1" & report != "Report 0") 
#head(df2c)
stats2c = stats.mean.sem(df2c, 6, 4)
stats2c = round(stats2c,1)
t2c = with(df2c, aov(WordCount ~ sem * report))
#summary(t2c)
p2c.sem = summary(t2c)[[1]][1,5]
p2c.report = summary(t2c)[[1]][2,5]
p2c.iteraction = summary(t2c)[[1]][3,5]

stats2c2 = stats.mean.sem(df2c, 6, c(3,4))
stats2c2 = round(stats2c2,1)

stats2c3 = stats.mean.sem(df2c, 6, 3)
stats2c3 = round(stats2c3,1)
t2c3 = t.test(df2c[,6] ~ df2c[,3])
#t2c3$p.value


df3c = df2 %>% filter(report == "Report 1" | report == "Report 2") 
#head(df3c)
t3c = with(df3c, aov(WordCount ~ course * report))
#summary(t3c)
p3c.course = summary(t3c)[[1]][1,5]
p3c.report = summary(t3c)[[1]][2,5]
p3c.iteraction = summary(t3c)[[1]][3,5]
if (round(p3c.course,3) <= 0.001) { p3c.course = "<0.001" }
if (round(p3c.report,3) <= 0.001) { p3c.report = "<0.001" }

df4 = subset(FbP.AcP, course == "Level 2")
df4 = df4 %>% select(SubmissionID, course, sem, report, project, Text, txt.words)
#head(df4)
t4 = with(df4, t.test(Text ~ report))
t4$p.value = tp(t4)

stats4 = stats.mean.sem(df4, 6, 4)
stats4 = round(stats4,1)

```


While audio annotations were relatively consistent in length, the typed annotations were not. The first year students in semester 1 received longer typed annotations on their early reports, with the longest typed comments appearing on the formative task (`r stats2a[1,1]`+/-`r stats2a[1,2]` words) and declining thereafter (Report 1: `r stats2a[2,1]`+/-`r stats2a[2,2]` words; Report 2: `r stats2a[3,1]`+/-`r stats2a[3,2]` words; Report 3: `r stats2a[4,1]`+/-`r stats2a[4,2]` words; p`r p2a`; Figure 3). In semester 2, where no formative task existed, the typed annotations declined in a similar way (Report 1: `r stats2b[1,1]`+/-`r stats2b[1,2]` words; Report 2: `r stats2b[2,1]`+/-`r stats2b[2,2]` words; Report 3: `r stats2b[3,1]`+/-`r stats2b[3,2]` words; p`r p2b`; Figure 3). Collectively, the amount of typed feedback provided across the first year reports was similar in each semester (Semester 1: `r stats2c3[1,1]`+/-`r stats2c3[1,2]` words; Semester 2: `r stats2c3[2,1]`+/-`r stats2c3[2,2]` words; p=`r round(t2c3$p.value, 3)`). For the second year reports, the number (Report 1: `r stats4[1,1]`+/-`r stats4[1,2]` annotations; Report 2: `r stats4[2,1]`+/-`r stats4[2,2]` annotations, p`r t4$p.value`) and length (Report 1: `r stats2[2,2]`+/-`r stats2[2,6]` words; Report 2: `r stats2[2,3]`+/-`r stats2[2,7]` words) of typed annotations declined (p`r p3c.report`) across the two reports. 


```{r, echo=FALSE}
df = FbP[,c(1,4:9,14:18)]
#head(df)   

df2 = FbU %>% select(SubmissionID, Pages)
df2 = unique(df2)
df2 = remover(df2, 2)
df3 = merge(df, df2)

#with(df3, table(Pages, Page))
#with(df3, round(addmargins(prop.table(table(Pages)))*100),4)
#with(df3, round(addmargins(prop.table(table(Page)))*100),4)

df3 = df3 %>% mutate(annot.pos = (Page/Pages)*100)
df3$annot.pos = round(df3$annot.pos,0)

#default.plot()
#hist(df3$annot.pos)

firstpage = round((prop.table(table(df3$Page == 1))*100),0)

df3 = df3 %>% mutate(lastpage = (Page == Pages)*100)
lastpage = round((prop.table(table(df3$lastpage))*100),2)
#lastpage[2]

nearlastpage = round((prop.table(with(df3, table(annot.pos >= 80)))*100),0)

```

The availability of in situ feedback possible with this marking tool was well utilised by markers, with annotations, whether typed or audio, being placed primarily in situ, and very few reports receiving ‘summary’ annotations either on the first (`r firstpage[2]`%) or last (`r lastpage[2]`%) page of the report text. This meant that each feedback annotation was placed near the specific portion of student work to which it referred. Previous studies investigating the impact of audio annotations have highlighted that students view the separation of audio comments from the relevant section of their work as confusing, for example when a single overall audio comment is provided, or when audio and text files are provided separately (Ribchester, France, and Wakefield 2008; Rodway-Dyer, Dunne, and Newcombe 2009). In this regard, the advent of completely online assessment submission and marking systems, which allow **in situ** embedding of feedback regardless of all modalities, represent a notable improvement in feedback provision.


```{r, echo=FALSE, fig.width=10, fig.height=5}

audio = subset(FbP, AnnotType == "Recording")
text = subset(FbP, AnnotType == "Text")

plot.topper.dual()
par(mar = c(4.5, 5.1, 2.1, 1.5))
plot.mean.sem(audio, 18, 14, 170, report.col, report.names, "Number of words per audio annotation")
plot.mean.sem(text, 18, 14, 17, report.col, report.names, "Number of words per typed annotation")

legend.top(sem.names,  kz.col)

temp = as.data.frame(table(FbP.AcP[,15], FbP.AcP[,17]))

#Kay wanted range of audio durations (sec) for later text
#temp3 = as.data.frame(tapply(FbP[,12], FbP[,14], mean, na.rm=T))
#temp4 = NULL
#for(i in 1:11)
#  temp4[[i]] = se(subset(FbP, project == project.names.formative[i]), 12)
#temp3
#temp4  

#Check stat's on decline across successive reports for review

##hacked
#a = subset(text, course == "Level 2")
#c = subset(a, sem =="Semester 2")
#with(c, t.test(WordCount ~ report))

##elegant
#with(subset(text, course == "Level 2" & sem == "Semester 1"), t.test(WordCount ~ report))

#with(subset(text, course == "Level 2" & sem == "Semester 1"), t.test(WordCount ~ report))

a = subset(text, report == "Report 3")
b = subset(text, course == "Level 2" & sem == "Semester 1")
ab = rbind(a,b)
#with(ab, t.test(WordCount ~ course))
  
```

Figure 3: Number of words provided in each audio and text annotation for level 1 course (Report 0: n=`r temp[1,3]`, Report 1: n=`r temp[3,3]`, Report 2: n=`r temp[5,3]`, Report 3: n=`r temp[7,3]`) and for the level 2 course (Report 1: n=`r temp[4,3]`, Report 2: n=`r temp[6,3]`). Shown is the A) mean+/-SEM number of words in each audio annotation in reports across two semesters and levels; B) mean+/-SEM number of words in each text annotation in reports across two semesters and two levels. *NB The y-axis scale for A is 10x the y-axis scale for B.*


```{r, echo=FALSE, fig.width=10, fig.height=5}

plot.topper.dual()

plot.mean.sem(FbP.AcP, 8, 14, 1700, report.col, report.names, "Number of words in audio annotations  \n per report")
plot.mean.sem(FbP.AcP, 9, 14, 170, report.col, report.names, "Number of words in typed annotations  \n per report")

legend.top(sem.names,  kz.col)

temp = as.data.frame(table(FbP.AcP[,15], FbP.AcP[,17]))

#stats.mean.sem(FbP.AcP, 9, 15)

```

Figure 4: Total amount of feedback provided in audio and text feedback for level 1 course (Report 0: n=`r temp[1,3]`, Report 1: n=`r temp[3,3]`, Report 2: n=`r temp[5,3]`, Report 3: n=`r temp[7,3]`) and for the level 2 course (Report 1: n=`r temp[4,3]`, Report 2: n=`r temp[6,3]`). Shown is the A) mean+/-SEM number of words in audio annotations in reports across two semesters and levels; B) mean+/-SEM number of words in text annotations in reports across two semesters and two levels. *NB The y-axis scale for A is 10x the y-axis scale for B.*


```{r, echo=FALSE, message=F, warning=F}

look = AcPFbUbins %>% select(StudentID, course, project, OpenDuration.min)
look = remover(look, 4)
u = which(look$OpenDuration.min == 0)
look = look[-u,]

d = dcast(look, StudentID ~ course, sum)

biol = d[which(d[,2] > 0),1]
biom = d[which(d[,3] > 0),1]

biol.all = demog %>% filter(course == "Level 1") %>% select(StudentID, sem, Course.Grade)
biom.all = demog %>% filter(course == "Level 2") %>% select(StudentID, sem, Course.Grade)

biol.looked = round(prop.table(table(biol.all[,1] %in% biol))*100,0)
biom.looked = round(prop.table(table(biom.all[,1] %in% biom))*100,0)


temp = with(subset(AcPFbUbins, course == "Level 1" & report == "Report 3"), round((prop.table(table(open.bin)))*100),2)
temp2 = 100-temp[4]

temp3 = with(subset(AcPFbUbins, course == "Level 2"), round((prop.table(table(open.bin, report),2))*100),2)
temp4 = 100-temp3[4,1]
temp5 = 100-temp3[4,2]

```

##Feedback use 
**_Proportion of students viewing report feedback_**  
The vast majority of the first year students opened their marked reports (`r biol.looked[2]`%), although the proportion who did so tended to decline slightly across the semester, with the final report being opened by fewer students (`r temp2`%; Figure 5). Surprisingly, the reverse was true in second year students. In comparison to the first year students, fewer second year students opened any of their reports (`r biom.looked[2]`%), with only `r temp4`% opening their first report and `r temp5`% opening their second report. Notably, this pattern was consistent in each semester (Figure 5). It is difficult to identify the cause of this difference between first and second year students, it may reflect a lower engagement with assessment by second year students (Loughlin et al. 2013) or could be that the additional opportunities students had during class to gain verbal feedback on their first report meant that they were less reliant on the feedback provided on the report documents. 


```{r, echo=FALSE, fig.width=10, fig.height=5}
looked = as.data.frame(addmargins(prop.table(table(AcPFbUbins[,6], AcPFbUbins[,11]),1)))
looked = round(100-(looked[37:47,3]*100),2)
#looked

default.plot()
barplot(looked, names = report.names, las=2, col=report.col, ylim=c(0,100), ylab="Number of Students (% of cohort)", axis.lty=1.0)
legend.top(sem.names,  kz.col)

temp = as.data.frame(table(AcP[,6]))
```


Figure 5: The proportion of students who looked at their feedback (shown as the proportion of students who received feedback in each cohort). Number of students in each cohort: Level 1 Semester 1: n=`r temp[1,2]`; Level 1 Semester 2: n=`r temp[5,2]`; Level 2 Semester 1: n=`r temp[8,2]`; Level 2 Semester 2: n=`r temp[10,2]`. 


```{r, echo=FALSE}
df = AcPFbUbins %>% filter(open.bin != "unopened")
#with(df, tapply(OpenDuration.min, final, mean, na.rm=T))
stats = stats.mean.sem(df, 10, 12)
stats = round(stats,0)

stats2 = stats.mean.sem(df, 10, c(12,7,8))
stats2 = round(stats2,0)

finalminmean = min(stats2[1,1:4])
w = which(stats2[1,1:4] == finalminmean)
s = as.matrix(stats2[1,5:8])
finalmaxmean = max(stats2[1,1:4])
w2 = which(stats2[1,1:4] == finalmaxmean)

nonfinalminmean = min(stats2[2,1:4])
w3 = which(stats2[2,1:4] == nonfinalminmean)
s2 = as.matrix(stats2[2,5:8])
nonfinalmaxmean = max(stats2[2,1:4])
w4 = which(stats2[2,1:4] == nonfinalmaxmean)

stats3maxmean = max(stats3[,1:2])
w2 = which(stats3[,1:2] == stats3maxmean)
#s[w2]

df2 = round(prop.table(table(AcPFbUbins[,12], AcPFbUbins[,11]),1)*100,0)
#df2[1,1]


```

**_Feedback viewing duration and pause times_**   
In terms of the duration for which student had reports open, in each semester and year level the duration declined markedly from averages ranging between `r round(nonfinalminmean/60,0)`-`r round(nonfinalmaxmean/60,0)` hours for the non-final reports, down to `r finalminmean`-`r finalmaxmean` minutes for the final report (Figure 6). 

When the patterns of open duration across the cohorts were examined in more detail (Figure 7), it was apparent that subsets of students interacted with their marked reports for distinct periods of time. For the non-final reports in each semester, the majority of students were divided approximately equally between those who opened their report for between one minute and an hour (`r df2[2,2]`%), and those who opened them for greater than an hour (`r df2[2,1]`%), with a smaller subset who opened their reports for less than 1 minute (`r df2[2,3]`%) or did not open their report at all (`r df2[2,4]`%). It is possible that these durations represent students engaging in different categories of behaviour (Warnakulasooriya et al 2007), students who work through the feedback thoroughly (moderate users); students who work through the feedback thoroughly and also use it directly to inform their subsequent report writing (long users); and a small tail of students who glance through their reports very quickly (short users), perhaps primarily to check their grade. This categorisation is supported by the timing and duration of openings across the assessment period. In the first year cohorts it was observed that there was a cluster of openings of marked reports shortly after their release and another cluster in the period 48 hours prior to the due date for the next report with the latter group being of longer open durations (data not shown). It is also supported by the pattern of open durations for the final reports from each semester, which have on average, much shorter open durations (Figure 6) and far greater proportions of students falling into the categories of shorter open durations, of less than one minute (`r df2[1,1]`%), or between one minute and one hour (`r df2[1,2]`%; Figure 7B & D). The duration and timing of students interaction with feedback occurring on the non-final tasks suggests that the students perceive these tasks to be sufficiently similar to one another for the feedback to be useful for the subsequent tasks (Boud and Molloy 2013) but also suggests that one of the key drivers for student interaction with feedback is the immediacy of its use on similar assessment tasks. 

**_need to add in P groups and convert dates_**  
This categorisation is supported by the timing of openings where it can be seen that there is a cluster of openings occurring shortly after the release of the marked reports (Figure X) and another cluster in the period 48 hours prior to the due date for the next report (Figure X) with the latter group being of longer open durations. It is also supported by the pattern of open durations for the final reports from each semester, which have on average, much shorter open durations (Figure 6) and far greater proportions of students falling into the categories of shorter open durations, of less than one minute (X%), or between one minute and one hour (X%; Figure 7B & D). The duration and timing of students interaction with feedback occurring on the non-final tasks suggests that the students perceive these tasks to be sufficiently similar to one another for the feedback to be useful for the subsequent tasks (Boud and Molloy 2013) but also suggests that one of the key drivers for student interaction with feedback is the immediacy of its use on similar assessment tasks. 


```{r, echo=FALSE, fig.width=10, fig.height=5}

default.plot()
plot.mean.sem.adj(AcPFbUbins, 10, 6, 8, report.col, report.names, "Open duration (hours)", 60)
legend.top(sem.names,  kz.col)

temp = as.data.frame(table(AcPFbUbins[,7], AcPFbUbins[,9]))

```


Figure 6: The duration students had their report feedback open for the level 1 course (Report 0: n=`r temp[1,3]`, Report 1: n=`r temp[3,3]`, Report 2: n=`r temp[5,3]`, Report 3: n=`r temp[7,3]`) and for the level 2 course (Report 1: n=`r temp[4,3]`, Report 2: n=`r temp[6,3]`). Data are expressed as the mean+/-SEM time for which each report was open, in hours.


```{r, echo=FALSE, fig.width=12, fig.height=5}

x = my.logs(-4, 8)
p = project.names.formative[c(1,4,8,9)]

default.plot()
par(mfrow=c(1,2))

for(i in 1:2) { 
  hist(log(subset(AcPFbUbins, project == p[i])[,10]), main = "", ylab = "Number of students", xlab = "Open duration (log scale)", xlim=c(-4,8),  ylim=c(0, 200), axes=F)
axis(1, at=seq(-4, 8, 1), labels = x)
axis(2)
}
for(i in 3:4) {
  hist(log(subset(AcPFbUbins, project == p[i])[,10]), main = "", ylab = "Number of students", xlab = "Open duration (log scale)", xlim=c(-4,8),  ylim=c(0, 50), axes=F)
axis(1, at=seq(-4, 8, 1), labels = x)
axis(2)
}


temp = as.data.frame(table(AcPFbUbins[,6]))

```

Figure 7: The number of students who had their feedback open for various durations following the Level 1 Semester 1 course A) formative report (Report 0: n=`r temp[1,2]`), and B) final report (Report 3: n=`r temp[4,2]`), and the Level 2 Semester 1 course C) first report (Report 1: n=`r temp[8,2]`), and C) final report (Report 2: n=`r temp[9,2]`). Data are presented as a frequency histogram showing the number of student across the log transformed open duration. 


```{r, echo=FALSE}
df = FbU %>% filter (msec.B4.scroll > 0)

stats3 = stats.mean.sem(df, 26, 27)
stats3 = as.data.frame(round(stats3,2))

stats4a = stats.mean.sem(subset(df, course == "Level 1"), 26, 27)
stats4a = as.data.frame(round(stats4a,2))
t4a = with(subset(df, course == "Level 1"), t.test(pause.min ~ final))

stats4b = stats.mean.sem(subset(df, course == "Level 2"), 26, 27)
stats4b = as.data.frame(round(stats4b,2))
t4b = with(subset(df, course == "Level 2"), t.test(pause.min ~ final))

pause.bin = c(0, 1, 180, max(df$pause.sec))
pauses = as.data.frame(round(prop.table(table(cut(df$pause.sec, pause.bin)))*100,0))

#*checking number of clicks for final vs nonfinal*  
#BIOL1040
df1 = FbU %>% filter(course == "Level 1", report != "Report 0" & report != "Report 2")
df1b = df1 %>% group_by(SubmissionID) %>% tally(sort=T)
#head(df1b)

finals = FbU %>% select(SubmissionID, report, final)
finals = (unique(finals))
df1b = merge(df1b, finals, by="SubmissionID")

stats = stats.mean.sem(df1b, 2, 4)
stats = round(stats,0)
#stats
t = with(df1b, t.test(n ~ final))
#t
t$p.value = tp(t)

#BIOM2011
df2 = FbU %>% filter(course == "Level 2")
df2b = df2 %>% group_by(SubmissionID) %>% tally(sort=T)
#head(df2b)

df2b = merge(df2b, finals, by="SubmissionID")

stats2 = stats.mean.sem(df2b, 2, 4)
stats2 = round(stats2,0)
#stats2
t2 = with(df2b, t.test(n ~ final))
#t2
t2$p.value = tp(t2)

```


Furthermore, the analytics used in this study are able to go beyond simple duration of openings, with the “clickstream” data providing the first insights into the temporal patterns of student interactions with feedback. These clicks represent student interactions with the report document, such as selecting a position within it, opening an audio annotation or scrolling. Students interacted with their non-final reports to a significantly greater degree compared with their final reports for both the level 1 course (Report 1: `r stats[2,1]`+/-`r stats[2,2]` clicks per report; Report 3: `r stats[1,1]`+/-`r stats[1,2]` clicks per report; p`r t$p.value`) and the level 2 course (Report 1: `r stats2[2,1]`+/-`r stats2[2,2]` clicks per report; Report 2: `r stats2[1,1]`+/-`r stats2[1,2]` clicks per report; p`r t2$p.value`). In addition, the pauses between clicks may provide a useful lens for understanding how students are interacting with their feedback. Minuscule pauses between scrolling clicks might indicate students accessing specific parts of their marked report, while short pauses between clicks are likely to represent students reading the feedback or their submission. Moderate pauses might indicate students working outside their feedback (for example on their next report), and very long pauses may indicate students leaving their feedback open in the background. On average, the amount of time students paused between clicks was `r stats3[2,1]`+/-`r stats3[2,2]` minutes for non-final reports, and `r stats3[1,1]`+/-`r stats3[1,2]` minutes for final reports (Figure 8). The vast majority of these pauses (`r pauses[2,2]`%) fell between 1 second and 3 minutes, with very few pauses exceeding one hour (`r pauses[3,2]`%; Figure 9). This finding suggests that, despite the often considerable duration for which students had their reports open, they were spending much of this time actively interacting with the report, rather than simply leaving it open for extended periods. This pattern of behaviour was also very consistent across all non-final reports at both year levels, such that it is likely that this represents the “normal” pattern by which students interact with *in situ* feedback. 

```{r, echo=FALSE, fig.width=10, fig.height=5}

default.plot()
plot.mean.sem(FbU, 26, 21, 4, report.col, report.names, "Pause duration (min)")
legend.top(sem.names,  kz.col)

temp = as.data.frame(table(AcPFbUbins[,7], AcPFbUbins[,9]))

```

Figure 8: Duration for which students pause between clicks when interacting with feedback for the level 1 course (Report 0: n=`r temp[1,3]`, Report 1: n=`r temp[3,3]`, Report 2: n=`r temp[5,3]`, Report 3: n=`r temp[7,3]`) and for the level 2 course (Report 1: n=`r temp[4,3]`, Report 2: n=`r temp[6,3]`). Data are expressed as the mean+/-SEM time that students paused between clicks for each report, in minutes


```{r, echo=FALSE, fig.width=10, fig.height=5}
df = FbU %>% filter (msec.B4.scroll > 0)

low = -5
high = 6
x = my.logs(low, high)

default.plot()
h = hist(log(df$pause.min), main = "", ylab = "Number of pauses", xlab = "Pause duration (log scale)", xlim=c(low, high),  ylim=c(0, 150000), axes=F)
axis(1, at=seq(low, high, 1), labels = x)
axis(2)

temp = length(unique(df$SubmissionID))
```

Figure 9: Frequency histogram of the duration students pause between clicks when interacting with feedback across the corpus of reports (n=`r temp` reports). Data are expressed as the number of times students paused between clicks for each duration (x scale is log transformed ranging from msec to hours).


```{r, echo=FALSE, fig.width=10, fig.height=5}
df = FbU %>% filter (msec.B4.scroll > 0)
df1 = df %>% filter(final == "nonfinal" )
df2 = df %>% filter(final == "final" )

low = -5
high = 6
x = my.logs(low, high)

default.plot()
par(mfrow=c(1,2))

h = hist(log(df1$pause.min), main = "", ylab = "Number of pauses", xlab = "Pause duration (log scale)", xlim=c(low, high),  ylim=c(0, 120000), axes=F)
axis(1, at=seq(low, high, 1), labels = x)
axis(2)

h = hist(log(df2$pause.min), main = "", ylab = "Number of pauses", xlab = "Pause duration (log scale)", xlim=c(low, high),  ylim=c(0, 120000), axes=F)
axis(1, at=seq(low, high, 1), labels = x)
axis(2)

temp1 = length(unique(df1$SubmissionID))
temp2 = length(unique(df2$SubmissionID))


```


Figure 9: Frequency histogram of the duration students pause between clicks when interacting with feedback for the A) non-final reports (n=`r temp1` reports) and B) final reports (n=`r temp2` reports). Data are expressed as the number of times students paused between clicks for each duration (x scale is log transformed ranging from msec to hours).


```{r, echo=FALSE, fig.width=10, fig.height=5}
df = FbU %>% filter (msec.B4.scroll > 0)
df1 = df %>% filter(final == "nonfinal" )
df2 = df %>% filter(final == "final" )

low = -5
high = 6
x = my.logs(low, high)
y = seq(0, 40, 10)
default.plot()
par(mfrow=c(1,2))

hist(log(df1$pause.min), main = "", ylab = "Proportion of pauses (%)", xlab = "Pause duration (log scale)", xlim=c(low, high),  ylim=c(0, 0.4), axes=F, prob = T)
axis(1, at=seq(low, high, 1), labels = x)
axis(2, at=seq(0, 0.4, 0.1), labels = y)

hist(log(df2$pause.min), main = "", ylab = "Proportion of pauses (%)", xlab = "Pause duration (log scale)", xlim=c(low, high),  ylim=c(0, 0.4), axes=F, prob = T)
axis(1, at=seq(low, high, 1), labels = x)
axis(2, at=seq(0, 0.4, 0.1), labels = y)

temp1 = length(unique(df1$SubmissionID))
temp2 = length(unique(df2$SubmissionID))


```


Figure 9: Frequency histogram of the duration students pause between clicks when interacting with feedback for the A) non-final reports (n=`r temp1` reports) and B) final reports (n=`r temp2` reports). Data are expressed as the proportion of pauses students took between clicks, ranging from milliseconds to hours (x scale is log transformed to improve clarity).



```{r, echo=FALSE, message=FALSE, warning=FALSE}

stats = stats.mean.sem(AcP, 5, 6)
stats = round(stats,1)
#stats

df11 = AcP %>% filter(course == "Level 1", sem == "Semester 1")
t11 = with(df11, aov(Final.Grade ~ report))
#summary(t11)
p11 = ap(t11)
tuk11 = TukeyHSD(t11)
#tuk11

df12 = AcP %>% filter(course == "Level 1", sem == "Semester 2")
t12 = with(df12, aov(Final.Grade ~ report))
#summary(t12)
p12 = ap(t12)
tuk12 = TukeyHSD(t12)
#tuk12

df2 = AcP %>% filter(course == "Level 2")
t2 = with(df2, t.test(Final.Grade ~ report))
#t2
t2$p.value = tp(t2)

df3 = AcP %>% filter(report == "Report 3" | project == "BIOM2011Sem1Report 2" | project == "BIOM2011Sem2Report 2")
t3 = with(df3, t.test(Final.Grade ~ course))
#t3
t3$p.value = tp(t3)

df4 = AcP %>% filter(course == "Level 1")
t4 = with(df4, aov(Final.Grade ~ project))
#summary(t4)
p4 = ap(t4)
tuk4 = TukeyHSD(t4)
#tuk4

#rownames(tuk4[[1]])
#c = length(rownames(tuk4[[1]]))
#x = c*3+1
#y = x+c-1
p4 = round(tuk4[[1]][72],2)
p5 = round(tuk4[[1]][65],2)
if(p5 <= 0.001) { p5 = "<0.001" }
#p5


```



##Academic Performance  
Both first and second year students showed a steady improvement in their performance on each report within each semester (Figure 10), with the average achievement on each subsequent task being significantly higher than the preceding task (Level 1 Semester 1: p`r p11`; Semester 2: p`r p12`; Level 2: p`r t2$p.value`). It should be noted that the performance on the first task in the second year course was significantly lower than that of the final report in the first year course (p`r t3$p.value`). Potentially this reflects the increase in assessment complexity and expectations across the year levels (Colthorpe et al. 2015). Interestingly, the average performance for the first year students in each semester was comparable between the summative tasks (p=`r p4`), but their performance on the formative task in semester 1 was lower than any summative tasks, regardless of semester (p`r p5`; Figure 10). This suggests that students in semester 2 are not disadvantaged by the lack of the formative task, potentially the experience they have gained in completing a semester prior to commencing this course has been an effective substitute for any learning gains from the formative task.



```{r, echo=FALSE, fig.width=10, fig.height=5}

default.plot()
plot.mean.sem(AcP, 5, 6, 100, report.col, report.names, "Report mark (%)")
legend.top(sem.names,  kz.col)

temp = as.data.frame(table(AcP[,7], AcP[,9]))

```


Figure 10: Overall final mark for reports in the level 1 course (Report 0: n=`r temp[1,3]`, Report 1: n=`r temp[3,3]`, Report 2: n=`r temp[5,3]`, Report 3: n=`r temp[7,3]`) and for the level 2 course (Report 1: n=`r temp[4,3]`, Report 2: n=`r temp[6,3]`). Data are expressed as the mean+/-SEM mark that students achieved for each report, as a percentage.


When student performance was compared to the extent to which they viewed their feedback, a number of differences were apparent both in terms of the average mark received on any given report and on changes between reports. In both first and second year students, all categories of students demonstrated significant, sequential improvements in average marks as they progressed between reports, with two exceptions (Figure 11). Students who only opened their feedback for a short duration showed no significant improvement in performance between reports, and those first year students who did not open their feedback improved between the first and second summative reports but then did not improve further (Figure 11). Generally, students who had their feedback open for medium or long durations had significantly higher marks than students who opened their feedback for short durations or those who did not open their feedback, with the long durations also out-performing the medium durations in later reports (Figure 11). While the overall improvement in performance across subsequent tasks cannot be attributed entirely to feedback, as the experience of completing one task is expected to contribute to a subsequent, similar task (ref), the relationships between differing durations of opening of reports and academic achievement does suggest that prolonged use of feedback is beneficial. Students who spend longer with their feedback open are more likely to have greater improvements in achievement on subsequent tasks than those who never open or only open their feedback briefly. 


```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=5}

source("AcP changes for diff open bins.R")
k = out.aov.acp.open()
temp = k[[1]][[5]][[3]]

temp2 = rbind(k[[1]][[1]][[3]], k[[1]][[2]][[3]], k[[1]][[3]][[3]], k[[1]][[4]][[3]])

open.bins = c("unopened", "short", "intermediate", "long")
#open.col = c("#FFFFFF", "#87CEFF", "#00BFFF", "#6495ED", "#4069E1", "#3D59AB")
open.col = c("#FFFFFF", "#87CEFA", "#6495ED", "#3D59AB")

#stats.R.alt = stats.R[c(seq(1,16,4), seq(2,16,4), seq(3,16,4), seq(4,16,4)),]

default.plot()
bars <- barplot(temp2$means, names = "", ylab = "Report mark (%)",  ylim =c(0,100), col = c(rep(open.col[1],4), rep(open.col[2],4), rep(open.col[3],4), rep(open.col[4],4)), las = 2)
for (i in 1:length(bars)) 
  {
    arrows(bars[i],temp2$means[i],bars[i],temp2$means[i]+temp2$SEM[i],angle=90,length=.09,lwd = 1.5)
    arrows(bars[i],temp2$means[i],bars[i],temp2$means[i]-temp2$SEM[i],angle=90,length=.09,lwd = 1.5)
}
axis(1, at = bars, labels = temp2$report, las = 2)
legend.top(open.bins,  open.col)

k[[1]][[2]][[2]]

```
Figure 11: Final marks for reports in the level 1, semester 1 course, with students categorised based on the duration for which they opened their Report 0 feedback into unopened (blue bars; n = `r temp[1,5]`), and short (green bars; <1 minute; n = `r temp[2,5]`), medium (yellow bars; >1 minute <1 hour; n = `r temp[3,5]`) and long (orange bars; >1 hour; n = `r temp[4,5]`) open durations. Data are expressed as the mean+/-SEM mark that students achieved for each report, as a percentage.


*We also investigated whether there were relationships between students’ academic performance, their interactions with the feedback and mark provision of feedback. Pioneering work in learning analytics using social networking analysis to investigate the interactions amongst students and staff in discussion boards indicates that academics interact more with high achieving students than with low achieving students (Dawson 2010; Dawson, Tan, and McWilliam 2011), In contrast, our analyses suggest that markers provide more feedback to students who perform poorly on their first report (Figure X). However this pattern breaks down for subsequent reports, where students who perform very poorly on report 2 receive the lowest amount of feedback (Figure X).*


```{r, echo=FALSE}
#proportion of audio played
#see
#"UQM paper - audio use.Rmd"
```


